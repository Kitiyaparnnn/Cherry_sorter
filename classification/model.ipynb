{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load image datasets\n",
    "def load_image_dataset(image_dir1, image_dir2, image_size=(224, 224)):\n",
    "    \"\"\"Loads image datasets from two directories with corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        image_dir1: Path to the first directory containing images.\n",
    "        image_dir2: Path to the second directory containing images.\n",
    "        image_size: Tuple specifying the desired image size (width, height).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the image data (NumPy array) and labels (NumPy array).\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(image_dir1):\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            filepath = os.path.join(image_dir1, filename)\n",
    "            try:\n",
    "                img = Image.open(filepath).convert(\"RGB\").resize(image_size)\n",
    "                img_array = np.array(img) / 255.0 # Normalize pixel values\n",
    "                images.append(img_array)\n",
    "                labels.append(0) # Label for images from the GOOD cherry\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "    for filename in os.listdir(image_dir2):\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            filepath = os.path.join(image_dir2, filename)\n",
    "            try:\n",
    "                img = Image.open(filepath).convert(\"RGB\").resize(image_size)\n",
    "                img_array = np.array(img) / 255.0\n",
    "                images.append(img_array)\n",
    "                labels.append(1) # Label for images from the BAD cherry\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "                \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Function to split dataset\n",
    "def split_dataset(images, labels, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"Splits the image dataset into training, testing, and validation sets.\"\"\"\n",
    "    x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=test_size, random_state=random_state, stratify=labels)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_size / (1 - test_size), random_state=random_state, stratify=y_train)\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 296\n",
      "Validation set size: 43\n",
      "Testing set size: 85\n"
     ]
    }
   ],
   "source": [
    "# Example usage (Replace with your actual directories)\n",
    "image_dir1 = os.getenv('GOOD_DATASET')\n",
    "image_dir2 = os.getenv('BAD_DATASET')\n",
    "images, labels = load_image_dataset(image_dir1, image_dir2)\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = split_dataset(images, labels)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"Training set size:\", len(x_train))\n",
    "print(\"Validation set size:\", len(x_val))\n",
    "print(\"Testing set size:\", len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 14:07:35.854296: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-11-16 14:07:35.854326: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-11-16 14:07:35.854333: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-11-16 14:07:35.854401: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-16 14:07:35.854438: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 14:07:36.439506: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - ETA: 0s - loss: 2.1207 - accuracy: 0.5000\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69767, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 14:07:38.031728: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/opt/anaconda3/envs/mlp/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 2s 84ms/step - loss: 2.1207 - accuracy: 0.5000 - val_loss: 0.5997 - val_accuracy: 0.6977\n",
      "Epoch 2/10\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5281 - accuracy: 0.7465\n",
      "Epoch 2: val_accuracy improved from 0.69767 to 0.83721, saving model to best_model.h5\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.5265 - accuracy: 0.7500 - val_loss: 0.4215 - val_accuracy: 0.8372\n",
      "Epoch 3/10\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4259 - accuracy: 0.8299\n",
      "Epoch 3: val_accuracy did not improve from 0.83721\n",
      "19/19 [==============================] - 1s 62ms/step - loss: 0.4176 - accuracy: 0.8345 - val_loss: 0.4769 - val_accuracy: 0.8140\n",
      "Epoch 4/10\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.3018 - accuracy: 0.8854\n",
      "Epoch 4: val_accuracy did not improve from 0.83721\n",
      "19/19 [==============================] - 1s 62ms/step - loss: 0.2988 - accuracy: 0.8851 - val_loss: 0.4097 - val_accuracy: 0.8372\n",
      "Epoch 5/10\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.2661 - accuracy: 0.8993\n",
      "Epoch 5: val_accuracy did not improve from 0.83721\n",
      "19/19 [==============================] - 1s 62ms/step - loss: 0.2659 - accuracy: 0.8986 - val_loss: 0.3765 - val_accuracy: 0.8372\n",
      "Epoch 6/10\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.2198 - accuracy: 0.8958\n",
      "Epoch 6: val_accuracy improved from 0.83721 to 0.86047, saving model to best_model.h5\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.2269 - accuracy: 0.8919 - val_loss: 0.3548 - val_accuracy: 0.8605\n",
      "Epoch 7/10\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.2887 - accuracy: 0.8854\n",
      "Epoch 7: val_accuracy did not improve from 0.86047\n",
      "19/19 [==============================] - 1s 63ms/step - loss: 0.2812 - accuracy: 0.8885 - val_loss: 0.4782 - val_accuracy: 0.8372\n",
      "Epoch 8/10\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.2055 - accuracy: 0.9201\n",
      "Epoch 8: val_accuracy did not improve from 0.86047\n",
      "19/19 [==============================] - 1s 63ms/step - loss: 0.2181 - accuracy: 0.9155 - val_loss: 0.3400 - val_accuracy: 0.8605\n",
      "Epoch 9/10\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.1835 - accuracy: 0.9271\n",
      "Epoch 9: val_accuracy improved from 0.86047 to 0.90698, saving model to best_model.h5\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.1966 - accuracy: 0.9223 - val_loss: 0.3191 - val_accuracy: 0.9070\n",
      "Epoch 10/10\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.2172 - accuracy: 0.9236\n",
      "Epoch 10: val_accuracy did not improve from 0.90698\n",
      "19/19 [==============================] - 1s 62ms/step - loss: 0.2136 - accuracy: 0.9257 - val_loss: 0.2416 - val_accuracy: 0.9070\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4767 - accuracy: 0.8235\n",
      "Test accuracy: 0.8235294222831726\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "def create_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(img_height, img_width, 3)),  # Input shape for RGB images\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(2, activation='sigmoid')  # Output layer for binary classification\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define image dimensions\n",
    "img_height = 224  # Example dimensions, adjust as needed\n",
    "img_width = 224\n",
    "\n",
    "# Create the model\n",
    "model = create_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define a ModelCheckpoint callback\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=16,\n",
    "          epochs=10,  # Adjust number of epochs as needed\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[checkpoint])\n",
    "\n",
    "# Evaluate the model\n",
    "_, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# predictions = model.predict(x_test)\n",
    "# disp = confusion_matrix(y_test, predictions, labels=[0,1])\n",
    "# disp.plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 15:27:02.522140: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict1: 1\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 15:27:02.791088: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict2: 1\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 15:27:03.059184: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict3: 1\n",
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 15:27:03.307943: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict4: 1\n",
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 15:27:03.564145: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict5: 1\n",
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 15:27:03.810549: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict6: 1\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Predict7: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 15:27:04.075699: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "path1 = \"./dataset/not_good/20231202_071734_2.jpg\" # green cherry\n",
    "path2 = \"./dataset/not_good/20231202_115412_2.jpg\" # white cherry\n",
    "path3 = \"./dataset/not_good/20231202_115756_1.jpg\" # rotten cherry\n",
    "path4 = \"./dataset/not_good/20231202_114425_2.jpg\" # yellow cherry\n",
    "path5 = \"./dataset/not_good/20231202_114447_1.jpg\" # half-red cherry\n",
    "path6 = \"bad_cherry.jpg\" # green sinjang\n",
    "path7 = \"./dataset/good/20231202_064819.jpg\" # red cherry\n",
    "\n",
    "paths = [path1,path2,path3,path4,path5,path6,path7]\n",
    "image_size = (224, 224)\n",
    "\n",
    "def prediction(path):\n",
    "    # Load model\n",
    "    model = keras.models.load_model(\"best_model.h5\")\n",
    "\n",
    "    # Load, convert, and resize the image\n",
    "    img = Image.open(path).convert(\"RGB\").resize(image_size)\n",
    "    img_array = np.array(img) / 255.0\n",
    "\n",
    "    # Add a batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    # print(\"image size: \", img_array.shape)\n",
    "\n",
    "    # Predict output\n",
    "    pred = 1 if model.predict(img_array) > 0.5 else 0\n",
    "    return pred\n",
    "\n",
    "for i in range(len(paths)): \n",
    "    pred = prediction(paths[i])\n",
    "    print(f\"Predict{i+1}: {pred}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to .tflite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/5t/qhv6pkk115b3_mpr1bbc5c9c0000gn/T/tmp3n0wx5vu/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/5t/qhv6pkk115b3_mpr1bbc5c9c0000gn/T/tmp3n0wx5vu/assets\n",
      "2024-11-16 15:30:50.909353: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2024-11-16 15:30:50.909366: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2024-11-16 15:30:50.909472: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/5t/qhv6pkk115b3_mpr1bbc5c9c0000gn/T/tmp3n0wx5vu\n",
      "2024-11-16 15:30:50.910395: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2024-11-16 15:30:50.910400: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /var/folders/5t/qhv6pkk115b3_mpr1bbc5c9c0000gn/T/tmp3n0wx5vu\n",
      "2024-11-16 15:30:50.913157: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2024-11-16 15:30:51.064835: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /var/folders/5t/qhv6pkk115b3_mpr1bbc5c9c0000gn/T/tmp3n0wx5vu\n",
      "2024-11-16 15:30:51.075408: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 165936 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Save the model as .tflite\n",
    "model = keras.models.load_model(\"best_model.h5\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with captured image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size:  (1, 224, 224, 3)\n",
      "model output:  [[0.15352581]]\n",
      "Prediction1: 0\n",
      "image size:  (1, 224, 224, 3)\n",
      "model output:  [[0.19350655]]\n",
      "Prediction2: 0\n",
      "image size:  (1, 224, 224, 3)\n",
      "model output:  [[0.25061744]]\n",
      "Prediction3: 0\n",
      "image size:  (1, 224, 224, 3)\n",
      "model output:  [[0.10779668]]\n",
      "Prediction4: 0\n",
      "image size:  (1, 224, 224, 3)\n",
      "model output:  [[0.25800547]]\n",
      "Prediction5: 0\n",
      "image size:  (1, 224, 224, 3)\n",
      "model output:  [[0.56318086]]\n",
      "Prediction6: 1\n",
      "image size:  (1, 224, 224, 3)\n",
      "model output:  [[0.7079757]]\n",
      "Prediction7: 1\n"
     ]
    }
   ],
   "source": [
    "path1 = \"./dataset/not_good/20231202_071734_2.jpg\" # green cherry\n",
    "path2 = \"./dataset/not_good/20231202_115412_2.jpg\" # white cherry\n",
    "path3 = \"./dataset/not_good/20231202_115756_1.jpg\" # rotten cherry\n",
    "path4 = \"./dataset/not_good/20231202_114425_2.jpg\" # yellow cherry\n",
    "path5 = \"./dataset/not_good/20231202_114447_1.jpg\" # half-red cherry\n",
    "path6 = \"bad_cherry.jpg\" # green sinjang\n",
    "path7 = \"./dataset/good/20231202_064819.jpg\" # red cherry\n",
    "\n",
    "paths = [path1,path2,path3,path4,path5,path6,path7]\n",
    "\n",
    "# Load the TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# print(\"input: \\n\", input_details)\n",
    "# print(\"output: \\n\", output_details)\n",
    "\n",
    "# Function for making predictions\n",
    "def predict_tflite(path):\n",
    "    # Load, convert, and resize the image\n",
    "    img = Image.open(path).convert(\"RGB\").resize(image_size)\n",
    "    img_array = np.array(img) / 255.0\n",
    "\n",
    "    # Add a batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0).astype(np.float32)\n",
    "    print(\"image size: \", img_array.shape)\n",
    "\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], img_array)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    print(\"model output: \",output_data)\n",
    "    pred = 1 if output_data > 0.5 else 0\n",
    "    return pred\n",
    "\n",
    "\n",
    "# Make the prediction\n",
    "for i in range(len(paths)):\n",
    "    pred = predict_tflite(paths[i])\n",
    "    print(f\"Prediction{i+1}: {pred}\") # O means good, 1 means bad\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
